
#+BEGIN_SRC python :noweb yes :tangle neural.py
import numpy as np
#import sklearn.dataset as data
import h5py
import tensorflow as tf
import sklearn

<<NeuralNet>>

#+END_SRC


* NeuralNet
#+NAME: NeuralNet
#+BEGIN_SRC python :noweb yes
class NeuralNet(object):
  <<NeuralNet_init>>
#+END_SRC
** __init__
#+NAME: NeuralNet_init
#+BEGIN_SRC python
  def __init__(self,
               input_node_size = None,               # Number of nodes in input layer
               output_node_size = None,              # Number of nodes in output layer
               hidden_layers_node_size = []          # Number of nodes in each hidden layer
              ):
      """

      init function for neuralNet

      Takes in the size of node layers (where each node layer is a number of nodes)
      Creates weight matrices and bias vectors with random values
      Function returns nothing.

      """

      # Randomize function seed
      np.random.seed(3)

      # Input checks
      if input_node_size is None or output_node_size is None:
          raise ValueError("input_node_size and output_node _size cannot be None")
      elif input_node_size < 1 or output_node_size < 1:
          raise ValueError("input_node_size and output_node_size must be greater than 0")

      # Creation of dimensions for weight matrices
      cols_size = [input_node_size] + hidden_layers_node_size
      rows_size = hidden_layers_node_size + [output_node_size]
      dimensions = zip(cols_size,rows_size)

      # Storage of weight and bias matrices
      self.weight_matrices = []
      self.bias_vectors = []

      for i, (col,row) in enumerate(dimensions):
          self.weight_matrices.append(
              np.random.randn(col,row) * 0.1
          )
          self.bias_vectors.append(
              np.random.randn(col,1)
          )
#+END_SRC

* L_model_forward
#+BEGIN_SRC python
def L_model_forward(X, parameters):
    """
    Implement forward propagation for the [LINEAR->RELU]*(L-1)->LINEAR->SIGMOID computation

    Arguments:
    X -- data, numpy array of shape (input size, number of examples)
    parameters -- output of initialize_parameters_deep()

    Returns:
    AL -- last post-activation value
    caches -- list of caches containing:
                every cache of linear_activation_forward() (there are L-1 of them, indexed from 0 to L-1)
    """

    <<linear_activation_forward>>

    caches = []
    A = X
    L = len(parameters) // 2                  # number of layers in the neural network

    # Implement [LINEAR -> RELU]*(L-1). Add "cache" to the "caches" list.
    for l in range(1, L):
        A_prev = A
        A, cache = linear_activation_forward(A_prev, parameters['W' + str(l)], parameters['b' + str(l)], activation = "relu")
        caches.append(cache)

    # Implement LINEAR -> SIGMOID. Add "cache" to the "caches" list.
    AL, cache = linear_activation_forward(A, parameters['W'+str(L)], parameters['b'+str(L)], activation = "sigmoid")
    caches.append(cache)

    assert(AL.shape == (1,X.shape[1]))

    return AL, caches
#+END_SRC
** linear_activation_forward
 Docs for sphinx
 #+NAME: linear_activation_forward
 #+BEGIN_SRC python :noweb yes
   def linear_activation_forward(A_prev,
                                 W,
                                 b,
                                 activation):
       """
       Implement the forward propagation for the LINEAR->ACTIVATION layer

       Arguments:
       A_prev -- activations from previous layer (or input data): (size of previous layer, number of examples)
       W -- weights matrix: numpy array of shape (size of current layer, size of previous layer)
       b -- bias vector, numpy array of shape (size of the current layer, 1)
       activation -- the activation to be used in this layer, stored as a text string: "sigmoid" or "relu"

       Returns:
       A -- the output of the activation function, also called the post-activation value
       cache -- a python dictionary containing "linear_cache" and "activation_cache";
                stored for computing the backward pass efficiently
       """

       #Define linear forward function, which is really just a way to multiply matrices together
       <<linear_forward>>

       #Apply linear_forward on the activation vector from the previous layer using the given weight matrix and bias vector.
       Z, linear_cache = linear_forward(A_prev, W, b)  

       # Apply the appropiate activation function
       if activation == "sigmoid":
           A, activation_cache = tf.nn.sigmoid(Z)
       elif activation == "relu":
           A, activation_cache = tf.nn.relu(Z)

       # Sanity check to ensure that the new shape is valid
       assert (A.shape == (W.shape[0], A_prev.shape[1]))


       cache = (linear_cache, activation_cache)
       return A, cache
 #+END_SRC

*** linear_forward

 Implements the linear part of a layer's forward propagation.

 Arguments:
 A -- activations from previous layer (or input data): (size of previous layer, number of examples)
 W -- weights matrix: numpy array of shape (size of current layer, size of previous layer)
 b -- bias vector, numpy array of shape (size of the current layer, 1)

 Returns:
 Z -- the input of the activation function, also called pre-activation parameter
 cache -- a python dictionary containing "A", "W" and "b" ; stored for computing the backward pass efficiently
     
  #+NAME: linear_forward
  #+BEGIN_SRC python
    def linear_forward(activation_vector,  # activations from previous layer
                       weight_matrix,  # weight matrix
                       bias_vector   # bias matrix
                      ):

        Z = np.dot( weight_matrix , activation_vector ) + bias_vector
        # Sanity Check to ensure that the result's shape is actually valid
        assert( Z.shape == (weight_matrix.shape[0], activation_vector.shape[1]) )
        cache = (activation_vector, weight_matrix, bias_vector)
        return Z, cache
  #+END_SRC

**** Tests

*** Tests


* compute_cost
#+BEGIN_SRC python
def compute_cost(AL, Y):
    """
    Implement the cost function defined by equation (7).

    Arguments:
    AL -- probability vector corresponding to your label predictions, shape (1, number of examples)
    Y -- true "label" vector (for example: containing 0 if non-cat, 1 if cat), shape (1, number of examples)

    Returns:
    cost -- cross-entropy cost
    """

    m = Y.shape[1]

    # Compute loss from aL and y.
    cost = -1/m*np.sum(np.multiply(Y, np.log(AL)) + np.multiply((1-Y), np.log(1-AL)))

    cost = np.squeeze(cost)      # To make sure your cost's shape is what we expect (e.g. this turns [[17]] into 17).
    assert(cost.shape == ())

    return cost
#+END_SRC

* L_model_backward
#+NAME: L_model_backward
#+BEGIN_SRC python
def L_model_backward(AL, Y, caches):
    """
    Implement the backward propagation for the [LINEAR->RELU] * (L-1) -> LINEAR -> SIGMOID group

    Arguments:
    AL -- probability vector, output of the forward propagation (L_model_forward())
    Y -- true "label" vector (containing 0 if non-cat, 1 if cat)
    caches -- list of caches containing:
                every cache of linear_activation_forward() with "relu" (it's caches[l], for l in range(L-1) i.e l = 0...L-2)
                the cache of linear_activation_forward() with "sigmoid" (it's caches[L-1])

    Returns:
    grads -- A dictionary with the gradients
             grads["dA" + str(l)] = ...
             grads["dW" + str(l)] = ...
             grads["db" + str(l)] = ...
    """
    grads = {}
    L = len(caches) # the number of layers
    m = AL.shape[1]
    Y = Y.reshape(AL.shape) # after this line, Y is the same shape as AL

    # Initializing the backpropagation
    dAL = - (np.divide(Y, AL) - np.divide(1 - Y, 1 - AL))

    # Lth layer (SIGMOID -> LINEAR) gradients. Inputs: "dAL, current_cache". Outputs: "grads["dAL-1"], grads["dWL"], grads["dbL"]
    grads["dA" + str(L-1)], grads["dW" + str(L)], grads["db" + str(L)] = linear_activation_backward(dAL, current_cache, activation = "sigmoid")

    # Loop from l=L-2 to l=0
    for l in reversed(range(L-1)):
        # lth layer: (RELU -> LINEAR) gradients.
        # Inputs: "grads["dA" + str(l + 1)], current_cache". Outputs: "grads["dA" + str(l)] , grads["dW" + str(l + 1)] , grads["db" + str(l + 1)]
        current_cache = caches[l]
        dA_prev_temp, dW_temp, db_temp = linear_activation_backward(grads["dA" + str(L-1)], caches[l], activation = "relu")
        grads["dA" + str(l)] = dA_prev_temp
        grads["dW" + str(l + 1)] = dW_temp
        grads["db" + str(l + 1)] = db_temp

    return grads
#+END_SRC

** linear_activation_backward
 #+NAME: linear_activation_backward
 #+BEGIN_SRC python
 def linear_activation_backward(dA, cache, activation):
     """
     Implement the backward propagation for the LINEAR->ACTIVATION layer.

     Arguments:
     dA -- post-activation gradient for current layer l
     cache -- tuple of values (linear_cache, activation_cache) we store for computing backward propagation efficiently
     activation -- the activation to be used in this layer, stored as a text string: "sigmoid" or "relu"

     Returns:
     dA_prev -- Gradient of the cost with respect to the activation (of the previous layer l-1), same shape as A_prev
     dW -- Gradient of the cost with respect to W (current layer l), same shape as W
     db -- Gradient of the cost with respect to b (current layer l), same shape as b
     """
     linear_cache, activation_cache = cache

     if activation == "relu":
         dZ = relu_backward(dA, activation_cache)
         dA_prev, dW, db = linear_backward(dZ, linear_cache)

     elif activation == "sigmoid":
         dZ = sigmoid_backward(dA, activation_cache)
         dA_prev, dW, db = linear_backward(dZ, linear_cache)

     return dA_prev, dW, db
 #+END_SRC

*** linear_backward
  #+NAME: linear_backward
  #+BEGIN_SRC python
  def linear_backward(dZ, cache):
      """
      Implement the linear portion of backward propagation for a single layer (layer l)

      Arguments:
      dZ -- Gradient of the cost with respect to the linear output (of current layer l)
      cache -- tuple of values (A_prev, W, b) coming from the forward propagation in the current layer

      Returns:
      dA_prev -- Gradient of the cost with respect to the activation (of the previous layer l-1), same shape as A_prev
      dW -- Gradient of the cost with respect to W (current layer l), same shape as W
      db -- Gradient of the cost with respect to b (current layer l), same shape as b
      """
      A_prev, W, b = cache
      m = A_prev.shape[1]

      dW = 1/m * np.dot(dZ, A_prev.T)
      db = 1/m*np.sum(dZ, axis = 1, keepdims = True)
      dA_prev = np.dot(W.T, dZ)

      assert (dA_prev.shape == A_prev.shape)
      assert (dW.shape == W.shape)
      assert (db.shape == b.shape)

      return dA_prev, dW, db
  #+END_SRC

* update_parameters
#+BEGIN_SRC python
def update_parameters(parameters, grads, learning_rate):
    """
    Update parameters using gradient descent

    Arguments:
    parameters -- python dictionary containing your parameters
    grads -- python dictionary containing your gradients, output of L_model_backward

    Returns:
    parameters -- python dictionary containing your updated parameters
                  parameters["W" + str(l)] = ...
                  parameters["b" + str(l)] = ...
    """

    L = len(parameters) // 2 # number of layers in the neural network

    # Update rule for each parameter. Use a for loop.
    for l in range(L):
        parameters["W" + str(l+1)] -= learning_rate*grads["dW" + str(l+1)]
        parameters["b" + str(l+1)] -= learning_rate*grads["db" + str(l+1)]
    return parameters

#+END_SRC

* relu_backward
#+BEGIN_SRC python
def relu_backward(dA, cache):
    """
    Implement the backward propagation for a single RELU unit.

    Arguments:
    dA -- post-activation gradient, of any shape
    cache -- 'Z' where we store for computing backward propagation efficiently

    Returns:
    dZ -- Gradient of the cost with respect to Z
    """

    Z = cache
    dZ = np.array(dA, copy=True) # just converting dz to a correct object.

    # When z <= 0, you should set dz to 0 as well.
    dZ[Z <= 0] = 0

    assert (dZ.shape == Z.shape)

    return dZ
#+END_SRC

* sigmoid_backward
#+BEGIN_SRC python
def sigmoid_backward(dA, cache):
    """
    Implement the backward propagation for a single SIGMOID unit.

    Arguments:
    dA -- post-activation gradient, of any shape
    cache -- 'Z' where we store for computing backward propagation efficiently

    Returns:
    dZ -- Gradient of the cost with respect to Z
    """

    Z = cache

    s = 1/(1+np.exp(-Z))
    dZ = dA * s * (1-s)

    assert (dZ.shape == Z.shape)

    return dZ

#+END_SRC

* predict
#+BEGIN_SRC python
def predict(X, Y, parameters):
    """
    This function is used to predict the results of a  L-layer neural network.

    Arguments:
    X -- data set of examples you would like to label
    Y -- labels from trained model
    parameters -- parameters of the trained model

    Returns:
    p -- predictions for the given dataset X
    """

    m = X.shape[1]
    n = len(parameters) // 2 # number of layers in the neural network
    p = np.zeros((1,m))

    # Forward propagation
    probas, caches = L_model_forward(X, parameters)


    # convert probas to 0/1 predictions
    for i in range(0, probas.shape[1]):
        if probas[0,i] > 0.5:
            p[0,i] = 1
        else:
            p[0,i] = 0

    #print results
    #print ("predictions: " + str(p))
    #print ("true labels: " + str(y))
    print("Accuracy: "  + str(np.sum((p == Y)/m)))

    return p
#+END_SRC
