
In order to use this file, activate org-babel for ipython and press C-c C-c to execute code blocks.

*Case setup*

First we import number and set its random seed to a fixed number for reproducibility.
We also set the keras backend to theano.

#+BEGIN_SRC ipython :session
import numpy as np

#Cc: idek <dhrmssrd68@gmail.com>
#Modified-by: Dharshan Ram <dhrmssrd68@gmail.com>

# For reproducibility
np.random.seed(123)

from keras import backend as K
import os

def set_keras_backend(backend):

    if K.backend() != backend:
        os.environ['KERAS_BACKEND'] = backend
        import importlib
        importlib.reload(K)
        assert K.backend() == backend
set_keras_backend("theano")
#+END_SRC

#+RESULTS:
: # Out[165]:

# Plotting the first image in the training data so that we have an idea of what we're looking at.
#+BEGIN_SRC ipython :session :results raw drawer :exports none
%matplotlib inline
# Visualize data
from matplotlib import pyplot as plt
# plt.imshow(X_train[0])
#+END_SRC

#+RESULTS:
:RESULTS:
# Out[129]:
:END:




Create and initialize the Neural Net
#+BEGIN_SRC ipython :noweb yes :session :exports none
# Class Definition of Neural Net
<<NeuralNet>>
#+END_SRC

#+RESULTS:
: # Out[186]:

#+BEGIN_SRC ipython :session
net = NeuralNet(
                output_node_size = 10,
                hidden_layers_node_size = [512])
#+END_SRC

#+RESULTS:
: # Out[187]:





Short script to look at the kind of data we have.
#+BEGIN_SRC sh :exports all
ls Data/
#+END_SRC

#+RESULTS:
| -10to0.csv        |
| -20to-10.csv      |
| -30to-20.csv      |
| -40to-30.csv      |
| -50to-40.csv      |
| -60to-50.csv      |
| 0to10.csv         |
| 10to20.csv        |
| 20to30.csv        |
| incrementby10.txt |
| incrementby2.txt  |
| incrementby20.txt |
| incrementby30.txt |
| incrementby40.txt |
| incrementby5.txt  |
| incrementby50.txt |
| random10reps.txt  |
| random5reps.text  |


# We must figure out a way to convert this output into a python list.
# The only downside is that we'll have to ensure that the folder only
# contains relevant csv files. The upsides are that we'll have excellent documentation for this.

#+BEGIN_SRC ipython :session
import csv
import pandas as pd
import numpy as np

from keras import preprocessing as pre

list_of_data_sources = ['Data/-60to-50.csv',
'Data/-50to-40.csv',
'Data/-40to-30.csv',
'Data/-30to-20.csv',
'Data/-20to-10.csv',
'Data/-10to0.csv',
'Data/0to10.csv',
'Data/10to20.csv',
'Data/20to30.csv',
'Data/incrementby10.txt',
'Data/incrementby20.txt',
'Data/incrementby2.txt',
'Data/incrementby30.txt',
'Data/incrementby40.txt',
'Data/incrementby50.txt',
'Data/incrementby5.txt',
'Data/random10reps.txt',
'Data/random5reps.text'
]

count = 1

'''
TODO: Need to find a way to use only 70% of data so 30$ can be used as validation
TODO: figure out what least common denom each len(X_data) has with each other so that we know ho many sequences we need


'''



for i in list_of_data_sources:

    with open(i, 'r') as csvfile:
    
        data = {'pitch':[],
                'drive':[],
                'input':[]}

        spamreader = csv.reader(csvfile, delimiter=',', quotechar='|')
        for row in spamreader:
            if len(row) is not 4:
                continue


            data['pitch'].append(row[0].split(' ')[-1])

            data['drive'].append(row[2].split(' ')[-1])
            data['input'].append(row[3].split(' ')[-1])
            
            #ummmmm = pre.sequence.TimeSeriesGenerator(data, ..., sampling_rate = 0.01, length = len(data['input']), start_index = 0, end_index = 7/10 * len(data))
        #print(data)
        pandas_frame = pd.DataFrame.from_dict(data)
        X_data = pandas_frame[['pitch','input']].values
        X_data = X_data.reshape(1, X_data.shape[0], X_data.shape[1], 1, 1)
        Y_Data = pandas_frame['drive'].values
        Y_Data = Y_Data.reshape(1,Y_Data.shape[0])

#    print(len(X_data))
#    print(len(Y_Data))

   
        #count+=1

        net.train(X_data,Y_Data,epochs=6)
#+END_SRC

#+RESULTS:
: # Out[172]:





#+BEGIN_SRC ipython :noweb yes :session :exports none
# Class Definition of PID
<<PID>>

#+END_SRC

#+RESULTS:
: # Out[32]:


#+BEGIN_SRC ipython :noweb yes :session :exports none
import random as rand
import time as t
"""

 we need something similar to mstimer2 to be able to make
 something work i believe. but the weird numbers achieved
 might also be the result of the randint doing whatever it wants... dunno

"""
def simulation(theta, pitch):
    p_term = 3
    i_term = 1.5
    d_term = 0.4
    angle_com = 0
    pid = PID(p_term=p_term,
              i_term=i_term,
              d_term=d_term,
              angle_com=angle_com
                    )
    pid.setup()
    # ppid.resetSystem()
    if theta >= pid.minAngle & theta <= pid.maxAngle:
        pid.controller.oldError = theta - pid.angle_com
        pid.controller.input_ = theta
        pid.updatePID(pitch)
    if pid.updatedPid:
        print("pitch: %f" % (pid.angle_com))
        print("\t")
        print("drive: %f" % (pid.drive))
        print("\t")
        print("input: %f" % (pid.controller.input_))
        pid.updatedPid = False

timeout = t.time() + 1

angle = rand.randint(-65, 30)

while True:

    pitch = rand.randint(-65 + 45, 10 - 25)
    simulation(angle, pitch)
    if t.time() > timeout:
        break


#+END_SRC

#+RESULTS:
: # Out[33]:








* NeuralNet
#+NAME: NeuralNet
#+BEGIN_SRC python :noweb yes :tangle neural.py
  class NeuralNet(object):

      def __init__(self,
                   input_node_size = None,               # Number of nodes in input layer
                   output_node_size = None,              # Number of nodes in output layer
                   input_shape = None,
                   hidden_layers_node_size = []          # Number of nodes in each hidden layer
                  ):
                      <<NeuralNet_init>>
      <<NeuralNet_train>>
      <<NeuralNet_run>>
      <<NeuralNet_label>>
#+END_SRC

** init

The Sequential model is a linear stack of layers. We pass in a list of layer instances to it to make a Neural Net.
#+NAME: NeuralNet_init
#+BEGIN_SRC python
          from keras.models import Sequential
          from keras import regularizers
          self.model = Sequential()
#+END_SRC

#+RESULTS: NeuralNet_init

Let's import the core layers from Keras which are almost always used.
#+NAME: NeuralNet_init
#+BEGIN_SRC python
          from keras.layers import Dense, Dropout, Activation, ConvLSTM2D, Flatten
#+END_SRC

The model should know what input shape it should expect. For this reason, we sepcifiy an input size for the first layer.
#+NAME: NeuralNet_init
#+BEGIN_SRC python
          # First layer requires input dimension ie input_shape
          self.model.add(
                         ConvLSTM2D(filters = 1, 
                                    kernel_size = (3, 3),
                                    kernel_initializer='random_uniform',
                                    bias_initializer='zeros',
                                    kernel_regularizer = regularizers.l2(.01),
                                    activity_regularizer = regularizers.l1(.01),
                                    return_sequences = True
                         
                               )
                         )
          self.model.add(Activation('relu'))
#+END_SRC

#+NAME: NeuralNet_init
#+BEGIN_SRC python

         #self.model.add(Flatten())

#+END_SRC

#+NAME: NeuralNet_init
#+BEGIN_SRC python
          # Add layers to model for all hidden layers
          for node_size in hidden_layers_node_size:
              self.model.add(
                             Dense(units=node_size)
                            )
              self.model.add(Activation('relu'))
              self.model.add(Dropout(0.3))
#+END_SRC

Adding a regularizer does not improve the model
#+NAME: NeuralNet_init
#+BEGIN_SRC python
#          from keras import regularizers
#          self.model.add(Dense(64,
#                          input_dim=64,
#                          kernel_regularizer=regularizers.l2(0.01),
#                          activity_regularizer=regularizers.l1(0.01))
#                   )
#+END_SRC

#+NAME: NeuralNet_init
#+BEGIN_SRC python
          # Last layer requires activation to be softmax
          self.model.add(
                         Dense(units=7854,
                               activation='softmax'
                               )
                        )
#+END_SRC


#+NAME: NeuralNet_init
#+BEGIN_SRC python
          # Compile model
          self.model.compile(loss='categorical_crossentropy',
                             optimizer='adam',
                             metrics=['accuracy'])
          #model.fit(x_train, y_train, epochs=5, batch_size=32)
#+END_SRC







** train

fit the model with training datasets

inputs:
train_x - training data
train_y - training labels
epochs - number of iterations over the entirity of both the x and y data desired

returns:
Nothing

#+NAME: NeuralNet_train
#+BEGIN_SRC python
    def train(self, train_x, train_y, epochs):
        self.model.fit(train_x, train_y, epochs)
#+END_SRC


** run


evaluates the model with test data

inputs:
X - test data
Y - test labels
steps - number of iterations over the entire dataset before evaluation is completed

returns:
metrics - the test losses as well as the metric defined in __init__, which in this case is accuracy

#+NAME: NeuralNet_run
#+BEGIN_SRC python
    def run(self, X, Y, steps):
        metrics = []
        metrics = self.model.evaluate(X, Y, batch_size = 32, steps = steps)
        return metrics
#+END_SRC


** label

predicts the labels of the data given

Inputs:
X - unlabeled test data
steps - number of iterations over the entire dataset before evaluation is completed

returns:
predictions - a numpy array of predictions

#+NAME: NeuralNet_label
#+BEGIN_SRC python
    def label(self, X, steps):
        predictions = self.model.predict(X, batch_size = 32, steps = steps)
        return predictions
#+END_SRC











* PID Controller

** PIDStruct


#+NAME: PID
#+BEGIN_SRC python :tangle pid.py
 """
 Class that acts as a mutable struct
 """
 class PIDStruct(object):
     def __init__(self, input_, Ki, Kp, Kd, oldError, dt, iState):
         self.input_ = input_
         self.Ki = Ki
         self.Kp = Kp
         self.Kd = Kd
         self.oldError = oldError
         self.dt = dt
         self.iState = iState
 #+END_SRC

** PID
#+NAME: PID
 #+BEGIN_SRC python :tangle pid.py :noweb yes

 """
 class where the PID is implemented
 """
 class PID(object):
     def __init__(self, p_term, i_term, d_term, angle_com):
         self.p_term = p_term
         self.i_term = i_term
         self.d_term = d_term
         self.controller = PIDStruct(0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00)
         self.min_i_term = -250
         self.max_i_term = 250
         self.angle_com = angle_com
         self.frequency = 100
         self.minAngle = -65
         self.maxAngle = 30
         self.maxFrequency = 1000
         self.buffersize = 2
         self.filteredVal = 0
         self.drive = 0
         self.index = 0
         self.updatedPid = False
         self.filterBuffer = [None] * self.buffersize

     <<PID_setup>>
     <<PID_resetsystem>>
     <<PID_updatePID>>
 #+END_SRC

*** setup
 #+NAME: PID_setup
  #+BEGIN_SRC python
  def setup(self):
      # arduino.close()
      # arduino = serial.Serial('/dev/cu.wchusbserial1420', 115200)
      # board.Servos.attach(Esc_pin)
      # board.pinMode(10, "OUTPUT")
      # board.digitalWrite(10, "LOW")
      self.controller.input_ = self.angle_com
      self.controller.Kp = self.p_term
      self.controller.Ki = self.i_term
      self.controller.Kd = self.d_term
      self.controller.dt = 1.0/self.frequency
      # arduino.write_line("press any key to arm or c to calibrate")
      # while arduino.in_waiting && arduino.read():
      # while !arduino.in_waiting
      # if arduino.read().decode('utf-8').lower() == "c":
      #     calibrate(Esc_pin)
      # else:
      #     arm(Esc_pin)
  #+END_SRC

*** resetsystem
 #+NAME: PID_resetsystem
  #+BEGIN_SRC python

  """
  Resets the PID controller to initialized state
  """

  def resetSystem(self):
      self.drive = 0
      self.updatedPid = False
      for i in range(0,self.buffersize):
          self.angle_com = 0
      self.controller.iState = 0
      self.controller.oldError = self.controller.input_ - self.angle_com
  #+END_SRC

*** updatePID
 #+NAME: PID_updatePID
  #+BEGIN_SRC python :noweb yes

 """
 updates PID values as soon as anew pitch request is made

 inputs:
 com - pitch request

 returns:
 updatedPid - boolean for if the PID has been updated or not
 """
 def updatePID(self, com):

     <<PID_trymap>>
     <<PID_constrain>>

     pTerm, iTerm, dTerm, error = 0,0,0,0
     self.angle_com = com
     error = self.controller.input_ - self.angle_com
     pTerm = self.controller.Kp * error
     self.controller.iState += error * self.controller.dt
     self.controller.iState = constrain(self.controller.iState, self.min_i_term/self.controller.Ki, self.max_i_term/self.controller.Ki)
     iTerm = self.controller.Ki * self.controller.iState
     dTerm = self.controller.Kd * ((error - self.controller.oldError) / self.controller.dt)
     self.drive = pTerm + iTerm + dTerm
     # setSpeed(Esc_pin, self.drive)
     self.updatedPid = True
     return self.drive
  #+END_SRC


**** trymap
  #+NAME: PID_trymap
   #+BEGIN_SRC python :tangle read.py
   """
   maps the given float to an integer value between out_min and out_max

   input:
   x - value to map
   in_min - min value that val is within, usually 0
   in_max - max value that val can be
   out_min - min value that val is to be mapped to
   out_max - max value that val is to be mapped to

   returns:
   mapped integer

   """
   def trymap(x, in_min, in_max, out_min, out_max):
       return int((x-in_min) * (out_max-out_min) / (in_max-in_min) + out_min)
   #+END_SRC

**** constrain
  #+NAME: PID_constrain
   #+BEGIN_SRC python :tangle read.py

   """
   constrains the value given to the range given

   input:
   val - the value to be constrained
   min_val - min value that val can be
   max_val - max valuse that val can be

   returns:
   value within the range given

   """
   def constrain(val, min_val, max_val):
       return min(max_val, max(min_val, val))
   #+END_SRC
