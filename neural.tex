% Created 2018-07-24 Tue 21:16
% Intended LaTeX compiler: pdflatex
\documentclass[11pt]{article}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{graphicx}
\usepackage{grffile}
\usepackage{longtable}
\usepackage{wrapfig}
\usepackage{rotating}
\usepackage[normalem]{ulem}
\usepackage{amsmath}
\usepackage{textcomp}
\usepackage{amssymb}
\usepackage{capt-of}
\usepackage{hyperref}
\author{grokkingStuff}
\date{\today}
\title{}
\hypersetup{
 pdfauthor={grokkingStuff},
 pdftitle={},
 pdfkeywords={},
 pdfsubject={},
 pdfcreator={Emacs 25.3.1 (Org mode 9.1.13)}, 
 pdflang={English}}
\begin{document}

\tableofcontents

In order to use this file, activate org-babel for ipython and press C-c C-c to execute code blocks.


First we import number and set its random seed to a fixed number for reproducibility.
\begin{verbatim}
import numpy as np
# For reproducibility
np.random.seed(123)

from keras import backend as K
import os

def set_keras_backend(backend):

    if K.backend() != backend:
        os.environ['KERAS_BACKEND'] = backend
        import importlib
        importlib.reload(K)
        assert K.backend() == backend

set_keras_backend("theano")
\end{verbatim}

Plotting the first image in the training data so that we have an idea of what we're looking at.
\begin{verbatim}
%matplotlib inline
# Visualize data
from matplotlib import pyplot as plt
# plt.imshow(X_train[0])
\end{verbatim}


\begin{verbatim}
net = NeuralNet(input_node_size = 784,
                output_node_size = 10,
                hidden_layers_node_size = [512])
\end{verbatim}

\begin{verbatim}
# net.train(X_train, Y_train, epochs=6)
\end{verbatim}



\section{NeuralNet}
\label{sec:org8ff7963}
\begin{verbatim}
class NeuralNet(object):

    def __init__(self,
                 input_node_size = None,               # Number of nodes in input layer
                 output_node_size = None,              # Number of nodes in output layer
                 input_shape = None,
                 hidden_layers_node_size = []          # Number of nodes in each hidden layer
                ):
                    from keras.models import Sequential
                    self.model = Sequential()
                    from keras.layers import Dense, Dropout, Activation, Flatten, LSTM
                    # First layer requires input dimension ie input_shape
                    self.model.add(
                                   LSTM(units=64,
                                         input_dim=input_node_size
                                         )
                                   )
                    self.model.add(Activation('relu'))
                    # Add layers to model for all hidden layers
                    for node_size in hidden_layers_node_size:
                        self.model.add(
                                       Dense(units=node_size)
                                      )
                        self.model.add(Activation('relu'))
                        self.model.add(Dropout(0.3))
                    #          from keras import regularizers
                    #          self.model.add(Dense(64,
                    #                          input_dim=64,
                    #                          kernel_regularizer=regularizers.l2(0.01),
                    #                          activity_regularizer=regularizers.l1(0.01))
                    #                   )
                    # Last layer requires activation to be softmax
                    self.model.add(
                                   Dense(units=output_node_size,
                                         activation='softmax'
                                         )
                                  )
                    # Compile model
                    self.model.compile(loss='categorical_crossentropy',
                                       optimizer='adam',
                                       metrics=['accuracy'])
                    #model.fit(x_train, y_train, epochs=5, batch_size=32)
    def train(self, train_x, train_y, epochs):
        self.model.fit(train_x, train_y, epochs, batch_size = 32)
    def run(self, X, Y, steps):
        metrics = []
        metrics = self.model.evaluate(X, Y, batch_size = 32, steps = steps)
        return metrics
    def label(self, X, steps):
        predictions = self.model.predict(X, batch_size = 32, steps = steps)
        return predictions
\end{verbatim}

\subsection{init}
\label{sec:org43c54a9}
The Sequential model is a linear stack of layers. We pass in a list of layer instances to it to make a Neural Net.
\begin{verbatim}
from keras.models import Sequential
self.model = Sequential()
\end{verbatim}

Let's import the core layers from Keras which are almost always used.
\begin{verbatim}
from keras.layers import Dense, Dropout, Activation, Flatten, LSTM
\end{verbatim}

The model should know what input shape it should expect. For this reason, we sepcifiy an input size for the first layer.
\begin{verbatim}
# First layer requires input dimension ie input_shape
self.model.add(
               LSTM(units=64,
                     input_dim=input_node_size
                     )
               )
self.model.add(Activation('relu'))
\end{verbatim}

\begin{verbatim}
# Add layers to model for all hidden layers
for node_size in hidden_layers_node_size:
    self.model.add(
                   Dense(units=node_size)
                  )
    self.model.add(Activation('relu'))
    self.model.add(Dropout(0.3))
\end{verbatim}

Adding a regularizer does not improve the model
\begin{verbatim}
#          from keras import regularizers
#          self.model.add(Dense(64,
#                          input_dim=64,
#                          kernel_regularizer=regularizers.l2(0.01),
#                          activity_regularizer=regularizers.l1(0.01))
#                   )
\end{verbatim}

\begin{verbatim}
# Last layer requires activation to be softmax
self.model.add(
               Dense(units=output_node_size,
                     activation='softmax'
                     )
              )
\end{verbatim}


\begin{verbatim}
# Compile model
self.model.compile(loss='categorical_crossentropy',
                   optimizer='adam',
                   metrics=['accuracy'])
#model.fit(x_train, y_train, epochs=5, batch_size=32)
\end{verbatim}






\subsection{train}
\label{sec:org6ee7a16}

fit the model with training datasets

inputs:
train\(_{\text{x}}\) - training data
train\(_{\text{y}}\) - training labels
epochs - number of iterations over the entirity of both the x and y data desired

returns:
Nothing

\begin{verbatim}
def train(self, train_x, train_y, epochs):
    self.model.fit(train_x, train_y, epochs, batch_size = 32)
\end{verbatim}

\subsection{run}
\label{sec:orge10a0b9}


evaluates the model with test data

inputs:
X - test data
Y - test labels
steps - number of iterations over the entire dataset before evaluation is completed

returns:
metrics - the test losses as well as the metric defined in \uline{\uline{init}}, which in this case is accuracy

\begin{verbatim}
def run(self, X, Y, steps):
    metrics = []
    metrics = self.model.evaluate(X, Y, batch_size = 32, steps = steps)
    return metrics
\end{verbatim}


\subsection{label}
\label{sec:org79da9c2}

predicts the labels of the data given

Inputs:
X - unlabeled test data
steps - number of iterations over the entire dataset before evaluation is completed

returns:
predictions - a numpy array of predictions

\begin{verbatim}
def label(self, X, steps):
    predictions = self.model.predict(X, batch_size = 32, steps = steps)
    return predictions
\end{verbatim}
\end{document}
